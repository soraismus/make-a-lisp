'use strict'

blank?       = require './isBlank'
{ jsArray? } = require './js-types'

{
  createStream!
  forEach
  prependValueToStream
  streamEnd?
} = require './stream-utilities'

{
  createMalHash
  createMalKeyword
  createMalList
  createMalNumber
  createMalString
  createMalSymbol
  createMalVector
  FALSE
  NIL
  TRUE
} = require './mal-types'

{ DEREF, QUASIQUOTE, QUOTE, SPLICE-UNQUOTE, UNQUOTE } =
  require './meta-symbols'

allSeqsAreCompleted? = \cache ->
  ! notAllSeqsAreCompleted? cache

atomize = \token ->
  switch
    when integerToken? token then createMalNumber (getIntegerValue token)
    when floatToken? token   then createMalNumber (getFloatValue token)
    when stringToken? token  then createMalString (getStringValue token)
    when keywordToken? token then createMalKeyword (getKeywordValue token)
    when nilToken? token     then NIL
    when trueToken? token    then TRUE
    when falseToken? token   then FALSE
    else                          createMalSymbol token

falseToken? = \token ->
  token == 'false'

floatToken? = \token ->
  /^-?[0-9]+.[0-9]+$/.test token

getAndValidateNextToken! = \getNextToken ->
  cache =
    '(' : 0
    '[' : 0
    '{' : 0
    ')' : 0
    ']' : 0
    '}' : 0

  stream = ->
    token = getNextToken ()
    increment (cache, token) unless streamEnd? token
    unless startTokensHavePreceded? cache
      throw 'Start tokens must precede end tokens'

    # TODO:
    #if !(start? token && allSeqsAreCompleted? cache)
    #  throw 'Error message TBD'

    if streamEnd? token && notAllSeqsAreCompleted? cache
      throw 'All seqs must be terminated with an appropriate end token'
    token

  (stream[key] = value) for own key, value of getNextToken

  stream

getCorrespondingMetaSymbol = \metaToken ->
  metaSymbols[metaToken]

getFloatValue = \value ->
  parseFloat (value, 10)

getIntegerValue = \token ->
  parseInt (token, 10)

getStringValue = \val ->
  #val.slice(1, val.length - 1)
  #   .replace(/\\"/g, '"')
  #   .replace(/\\n/g, '"')
  val.slice(1, val.length - 1)

getSeqConstructor = \endToken ->
  seqTypeConstructors[endToken] ??  \value -> throw 'Invalid end token'

increment = (cache, token) ->
  cache[token]++

integerToken? = \token ->
  /^-?[0-9]+$/.test token

getKeywordValue = \val ->
  val[1..]

keywordToken? = \token ->
  colon = ':'
  token[0] == colon

stringToken? = \token ->
  doubleQuote = '"'
  token[0] == doubleQuote

available? = \str ->
  ! blank? str

comment? = \token ->
  token[0] == commentStart

end? = \token ->
  token in endTokens

# no-op
ignore! = (stack, token) ->

meta? = \token ->
  token in metaTokens

metadataMarker? = \token ->
  token == metadataMarker

nilToken? = \token ->
  token == 'nil'

notAllSeqsAreCompleted? = \cache ->
  cache[listStart] != cache[listEnd] ||
    cache[vectorStart] != cache[vectorEnd] ||
    cache[hashmapStart] != cache[hashmapEnd]

_parse = (token, tokens) ->
  switch
    when token == hashmapStart then parseHash tokens
    when token == listStart    then parseList tokens
    when token == vectorStart  then parseVector tokens
    when meta? token           then parseMeta (token, tokens)
    else                            atomize token

parse = \tokens ->
  _parse ((shift tokens), tokens)

parseMeta = (metaToken, tokens) ->
  createMalList [
    getCorrespondingMetaSymbol metaToken  
    parse tokens
  ]

parseSeq = (seqEnd, createMalSeq) -> (tokens) ->
  array = []
  until (token = shift tokens) == seqEnd
    array.push (_parse (token, tokens))
  createMalSeq array

# TODO: Separate `tokenize` and `parse` into separate files.
read = \str ->
  parse (tokenize str)

shift = \queue ->
  queue.shift ()

start? = \token ->
  token in startTokens

startTokensHavePreceded? = \cache ->
  cache[listStart] >= cache[listEnd] &&
    cache[vectorStart] >= cache[vectorEnd] &&
    cache[hashmapStart] >= cache[hashmapEnd]

tokenize = \str ->
  tokenRegex =
    /[\s,]*(~@|[\[\]{}()'`~^@]|"(?:\\.|[^\\"])*"|;.*|[^\s\[\]{}('"`,;)]*)/g
  tokens = []
  while available? (token = tokenRegex.exec(str)[1])
    continue if comment? token
    tokens.push token
  tokens

trueToken? = \token ->
  token == 'true'

listStart      = '('
listEnd        = ')'
vectorStart    = '['
vectorEnd      = ']'
hashmapStart   = '{'
hashmapEnd     = '}'
commentStart   = ';'
deref          = '@'
quote          = '\'' 
quasiquote     = '`'
unquote        = '~'
spliceUnquote  = '~@'
metadataMarker = '^'

startTokens = [listStart, vectorStart, hashmapStart]
endTokens   = [listEnd, vectorEnd, hashmapEnd]
metaTokens  = [deref, quote, quasiquote, unquote, spliceUnquote]

metaSymbols =
  '@'  : DEREF
  '`'  : QUASIQUOTE
  '\'' : QUOTE
  '~@' : SPLICE-UNQUOTE
  '~'  : UNQUOTE

seqTypeConstructors =
  ')' : createMalList
  ']' : createMalVector
  '}' : createMalHash

parseHash   = parseSeq (hashmapEnd, createMalHash)
parseList   = parseSeq (listEnd, createMalList)
parseVector = parseSeq (vectorEnd, createMalVector)

module.exports = read
